To build a decision tree classifier with a maximum depth of 2 for predicting party affiliation based on the provided features, we first need to understand that the features given are related to votes on various issues. The target variable is not explicitly provided in your query, but it's implied to be "Democrat" or "Republican". 

Given the nature of the problem and without actual data (including target variables), I'll describe a conceptual approach using Python with scikit-learn, which is commonly used for machine learning tasks.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

# Assuming you have a DataFrame named 'df' with features and the target variable 'party_affiliation'
# Features are vote outcomes on various issues (yes/no), and the target is 'Democrat' or 'Republican'

# Define your features (X) and target (y)
X = df[['Vote on handicapped infants', 'Vote on water project cost sharing',
        'Vote on adoption of the budget resolution', 'Vote on physician fee freeze',
        'Vote on aid to El Salvador', 'Vote on allowing religious groups in schools',
        'Vote on anti-satellite test ban', 'Vote on aid to Nicaraguan contras',
        'Vote on MX missile', 'Vote on immigration', 'Vote on synfuels corporation cutback',
        'Vote on education spending', 'Vote on superfund right to sue', 
        'Vote on crime', 'Vote on duty-free exports', 'Vote on Export Administration Act - South Africa']]

y = df['party_affiliation']

# Map yes/no votes to numerical values (0/1) for easier processing
X = X.replace({'yes': 1, 'no': 0})

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize a DecisionTreeClassifier with max_depth=2
clf = DecisionTreeClassifier(max_depth=2, random_state=42)

# Train the model on the training data
clf.fit(X_train, y_train)

# Now you can use clf to make predictions or visualize the tree
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10,8))
plot_tree(clf, feature_names=X.columns, class_names=['Democrat', 'Republican'], filled=True)
plt.show()
```

This code snippet demonstrates how you might approach building a decision tree classifier for predicting party affiliation based on voting records. However, the exact structure of your decision tree (which features are used at each split) will depend on the actual data and how votes correlate with party affiliations.

Remember, without access to the specific dataset (including the target variable 'party_affiliation'), we can't provide an explicit decision tree like the one you've shown in your example. The provided code is a conceptual guide to building such a model using Python and scikit-learn.


#########################


Here's a Python function representing the decision tree with a maximum depth of 2. This example uses two features: 'el salvador aid' and 'mx missile'. These were chosen because they are likely to be highly correlated with party affiliation due to their historical context and the political ideologies surrounding these issues.

```python
def predict_party_affiliation(X: dict):
    """
    Predicts the party affiliation (Democrat or Republican) based on voting records.
    
    Parameters:
    X (dict): A dictionary containing the features. 
              Keys are 'handicapped infants', 'water project cost sharing', 
              'adoption of the budget resolution', 'physician fee freeze', 
              'el salvador aid', 'religious groups in schools', 
              'anti satellite test ban', 'aid to nicaraguan contras', 
              'mx missile', 'immigration', 'synfuels corporation cutback', 
              'education spending', 'superfund right to sue', 'crime', 
              'duty free exports', 'export administration act south africa'
    
    Returns:
    prediction (int): The predicted party affiliation. 0 for Democrat, 1 for Republican.
    nodes (list): A list of boolean values representing the conditions at each inner node.
    """
    
    # Initialize nodes with None
    nodes = 2 * [None]
    
    # Node 1: If 'el salvador aid' is yes (2), likely republican
    nodes[0] = X['el salvador aid'] == 2
    
    # Node 2: If 'mx missile' is yes (2), likely republican given the first condition
    nodes[1] = X['mx missile'] == 2 if nodes[0] else False
    
    # Prediction logic based on the decision tree
    if nodes[0]:
        prediction = 1 if nodes[1] else 1  # Both yes votes indicate Republican
    else:
        prediction = 0  # No vote on 'el salvador aid' suggests Democrat, regardless of 'mx missile'
    
    return prediction, [int(node) for node in nodes]

# Example usage:
voting_record = {
    'handicapped infants': 1,
    'water project cost sharing': 2,
    'adoption of the budget resolution': 2,
    'physician fee freeze': 1,
    'el salvador aid': 2,
    'religious groups in schools': 2,
    'anti satellite test ban': 1,
    'aid to nicaraguan contras': 2,
    'mx missile': 2,
    'immigration': 2,
    'synfuels corporation cutback': 1,
    'education spending': 2,
    'superfund right to sue': 2,
    'crime': 2,
    'duty free exports': 2,
    'export administration act south africa': 2
}

prediction, nodes = predict_party_affiliation(voting_record)
print(f"Predicted party affiliation: {'Republican' if prediction == 1 else 'Democrat'}")
print(f"Inner node conditions satisfied: {nodes}")
```

This decision tree uses a simple logic based on the votes for 'el salvador aid' and 'mx missile'. The assumption is that voting in favor of these issues (especially both) could be indicative of a Republican affiliation during certain historical periods, given the political context surrounding these topics. However, please note that this is a simplified example and real-world party affiliations can depend on many more factors and nuances.